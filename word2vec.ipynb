{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 数据集纠错处理\r\n",
    "本示例用到的数据文件来源于http://statmt.org/wmt18/translation-task.html#download 中的News Commentary v13\r\n",
    "由于中文会部分出现乱码，需要将对应的数据丢弃\r\n",
    "\r\n",
    "数据集是gbk的，在读写操作时需要转成utf-8"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "# 处理问题数据\r\n",
    "# data_en = []\r\n",
    "# data_cn = []\r\n",
    "# with open(\"(origin)news-commentary-v13.zh-en.en\",\"r\",encoding='utf-8') as f:\r\n",
    "#     data_en = f.read().split('\\n')\r\n",
    "\r\n",
    "# with open(\"(origin)news-commentary-v13.zh-en.zh\",\"r\",encoding='utf-8') as f:\r\n",
    "#     data_cn = f.read().split('\\n')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "#   将没问题的数据单独挑出来\r\n",
    "# data_cn_corrected = []\r\n",
    "# data_en_corrected = []\r\n",
    "# for i in range(data_cn.__len__()):\r\n",
    "#     if data_cn[i].__contains__(\"�\"):\r\n",
    "#         continue\r\n",
    "#     data_en_corrected.append(data_en[i])\r\n",
    "#     data_cn_corrected.append(data_cn[i])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "#   将处理过的数据存进新文件\r\n",
    "# with open(\"news-commentary-v13.zh-en.en\",\"w\",encoding='utf-8') as f:\r\n",
    "#     for i in data_en_corrected:\r\n",
    "#         f.write(i+'\\n')\r\n",
    "\r\n",
    "\r\n",
    "# with open(\"news-commentary-v13.zh-en.cn\",\"w\",encoding='utf-8') as f:\r\n",
    "#     for i in data_cn_corrected:\r\n",
    "#         f.write(i+'\\n')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word Embedding词嵌入\r\n",
    "### 本节可能涉及的知识点：\r\n",
    ">获取上下文关系</br>\r\n",
    ">   &emsp;Skip-Gram(本例采用)</br>\r\n",
    ">   &emsp;CBOW</br>\r\n",
    ">因词库太大而不能直接使用softmax输出，对应的处理方式</br>\r\n",
    ">   &emsp;基于Huffman tree的Hierarchical Softmax方法</br>\r\n",
    ">   &emsp;负采样negative sampling(本例采用)</br>\r\n",
    ">分词</br>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "# 复现文章reference:\r\n",
    "# Distributed Representations of Words and Phrases and their Compositionality\r\n",
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "import torch.nn.functional as fun\r\n",
    "from torch.utils.data import Dataset\r\n",
    "from torch.utils.data import DataLoader\r\n",
    "\r\n",
    "from collections import Counter\r\n",
    "import numpy as np\r\n",
    "import random\r\n",
    "import math\r\n",
    "import jieba #中文分词工具\r\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 一些全局配置变量"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "USE_CUDA = torch.cuda.is_available()\r\n",
    "\r\n",
    "contextWindow = 3\r\n",
    "K = 100 # 反例相比正例的倍数\r\n",
    "\r\n",
    "NUM_EPOCHS = 2\r\n",
    "BATCH_SIZE = 256\r\n",
    "LEARNING_RATE = 0.2\r\n",
    "EMBEDDING_SIZE = 120"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 分别对原始数据中的中文和英文进行分词处理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "with open(\"news-commentary-v13.zh-en.cn\",\"r\",encoding='utf-8') as f:\r\n",
    "    text_cn = f.read()\r\n",
    "\r\n",
    "with open(\"news-commentary-v13.zh-en.en\",\"r\",encoding='utf-8') as f:\r\n",
    "    text_en = f.read()\r\n",
    "\r\n",
    "#   当不给split函数传递任何参数时，分隔符sep会采用任意形式的空白字符\r\n",
    "text_en = text_en.split()   #英文单词直接用空格分就行\r\n",
    "text_cn = jieba.lcut(text_cn,cut_all=False)   #用jieba的精确模式对中文进行分词\r\n",
    "\r\n",
    "vocab_en = dict(Counter(text_en))\r\n",
    "vocab_cn = dict(Counter(text_cn))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "创建两组映射对应英\\中文词汇表中序号-单词的对应关系"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "mappingCN_index2word = [word for word in vocab_cn.keys()]\r\n",
    "mappingCN_word2index = {word:i for i,word in enumerate(mappingCN_index2word)}\r\n",
    "\r\n",
    "mappingEN_index2word = [word for word in vocab_en.keys()]\r\n",
    "mappingEN_word2index = {word:i for i,word in enumerate(mappingEN_index2word)}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "获取每个词语出现的频率的3/4次幂，再归一化"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [
    "cn_word_counts = np.array([count for count in vocab_cn.values()],dtype=np.float32)\r\n",
    "en_word_counts = np.array([count for count in vocab_en.values()],dtype=np.float32)\r\n",
    "\r\n",
    "cn_word_freq = (cn_word_counts / np.sum(cn_word_counts) ** (3./4.))\r\n",
    "cn_word_freq = cn_word_freq / np.sum(cn_word_freq)  #   normalization\r\n",
    "\r\n",
    "en_word_freq = (en_word_counts / np.sum(en_word_counts) ** (3./4.))\r\n",
    "en_word_freq = en_word_freq / np.sum(en_word_freq)  #   normalization"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 构建嵌入模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "class EmbeddingModel (nn.Module):\r\n",
    "    def __init__(self, vocab_size, embed_size):\r\n",
    "        super (EmbeddingModel, self).__init__()\r\n",
    "\r\n",
    "        #   这为啥非得分两个网络\r\n",
    "        self.vocab_size = vocab_size    #词库总长度\r\n",
    "        self.embed_size = embed_size    #嵌入后词向量的维度\r\n",
    "\r\n",
    "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\r\n",
    "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\r\n",
    "    def forward(self, input_labels, pos_labels, neg_labels):\r\n",
    "        # input_label: [batch_size]\r\n",
    "        # pos_labels: [batch_size, (window_size * 2)]\r\n",
    "        # neg_labels: [batch_size, (window_size * 2 * K)]\r\n",
    "        input_embedding = self.in_embed (input_labels)  # [batch_size, embed_size]\r\n",
    "        pos_embedding = self.out_embed (pos_labels)  # 正例 [batch_size, (window_size 2), embed_size]\r\n",
    "        neg_embedding = self.out_embed (neg_labels)  # 反例 [batch_size, (window_size 2 K), embed_size]\r\n",
    "\r\n",
    "        # unsquuze(x) 是在第x维增加一个维度 squeeze与之相反\r\n",
    "        input_embedding = input_embedding.unsqueeze (2) # [batch_size, embed size, 1]\r\n",
    "\r\n",
    "        #bmm  Batch Map Mul\r\n",
    "        pos_dot = torch.bmm(pos_embedding, input_embedding).squeeze(2)  # [batch_size, (window_size * 2)]\r\n",
    "        neg_dot = torch.bmm(neg_embedding, -input_embedding).squeeze(2)  # [batch_size, (window_size * 2 * K)]\r\n",
    "\r\n",
    "        # 这为啥用logsigmoid我还不知道，论文里是这么写的\r\n",
    "        log_pos = fun.logsigmoid(pos_dot).sum(1)    # 把所有第一维元素加和\r\n",
    "        log_neg = fun.logsigmoid(neg_dot).sum(1)\r\n",
    "\r\n",
    "        loss = log_pos + log_neg\r\n",
    "        return -loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 定义英文/中文词汇数据集"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "class WordEmbeddingDataset (Dataset):\r\n",
    "    #   词汇列表 两个映射列表 词频列表 \r\n",
    "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\r\n",
    "        super(WordEmbeddingDataset, self). __init__()\r\n",
    "        self.text_encoded= [word_to_idx.get(word) for word in text] # 返回指定键的值，如果值不在字典中返回default值\r\n",
    "        self.text_encoded = torch.LongTensor(self.text_encoded)\r\n",
    "        self.word_to_idx = word_to_idx\r\n",
    "        self.idx_to_word = idx_to_word\r\n",
    "        self.word_freqs = torch.Tensor(word_freqs)\r\n",
    "        self.word_counts = torch.Tensor (word_counts)   #   貌似没用\r\n",
    "           \r\n",
    "    def __len__(self):\r\n",
    "        # 这个数据集一共有多少个item\r\n",
    "        return len(self.text_encoded)\r\n",
    "           \r\n",
    "    def __getitem__ (self, idx):\r\n",
    "        center_word = self.text_encoded[idx]\r\n",
    "        pos_indices = list(range(idx-contextWindow,idx))+list(range (idx + 1, idx+contextWindow+1))#   window内单词的index\r\n",
    "        pos_indices = [i % len(self.text_encoded) for i in pos_indices] #   处理词表两端越界问题 比如就一万个词，上界到一万零二的情况\r\n",
    "        pos_words =self.text_encoded[pos_indices]\r\n",
    "\r\n",
    "        #   以word_freqs内元素的值作为概率，从word_freqs的index中抽取K*pos_words.shape[0]个值，有放回\r\n",
    "        neg_words = torch.multinomial(self.word_freqs, K*pos_words.shape[0], True)   # 负例采样单词\r\n",
    "        return center_word, pos_words, neg_words\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "dataset_cn = WordEmbeddingDataset (text_cn, mappingCN_word2index, mappingCN_index2word, cn_word_freq, cn_word_counts)\r\n",
    "dataset_en = WordEmbeddingDataset (text_en, mappingEN_word2index, mappingEN_index2word, en_word_freq, en_word_counts)\r\n",
    "dataloader_cn = DataLoader(dataset_cn, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\r\n",
    "dataloader_en = DataLoader(dataset_en, batch_size = BATCH_SIZE, shuffle = True, num_workers = 0)\r\n",
    "\r\n",
    "model_cn = EmbeddingModel(len(vocab_cn),EMBEDDING_SIZE)\r\n",
    "model_en = EmbeddingModel(len(vocab_en),EMBEDDING_SIZE)\r\n",
    "if USE_CUDA:\r\n",
    "    model_cn = model_cn.cuda()\r\n",
    "    model_en = model_en.cuda()\r\n",
    "\r\n",
    "\r\n",
    "optimizer_cn = torch.optim.SGD(model_cn.parameters(),lr=LEARNING_RATE)\r\n",
    "optimizer_en = torch.optim.SGD(model_en.parameters(),lr=LEARNING_RATE)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 训练并保存模型"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if __name__ == '__main__':\r\n",
    "    for e in range(NUM_EPOCHS):\r\n",
    "        for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader_cn):\r\n",
    "            input_labels = input_labels.long().cuda()\r\n",
    "            pos_labels = pos_labels.long().cuda()\r\n",
    "            neg_labels = neg_labels.long().cuda()\r\n",
    "\r\n",
    "            optimizer_cn.zero_grad()\r\n",
    "            loss = model_cn(input_labels,pos_labels,neg_labels).mean()\r\n",
    "            loss.backward()\r\n",
    "            optimizer_cn.step()\r\n",
    "\r\n",
    "            if i % 100 == 0:\r\n",
    "                print(\"cn_epoch\",e,\"iteration\",i,loss.item())\r\n",
    "\r\n",
    "        for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader_en):\r\n",
    "            input_labels = input_labels.long().cuda()\r\n",
    "            pos_labels = pos_labels.long().cuda()\r\n",
    "            neg_labels = neg_labels.long().cuda()\r\n",
    "\r\n",
    "            optimizer_en.zero_grad()\r\n",
    "            loss = model_en(input_labels,pos_labels,neg_labels).mean()\r\n",
    "            loss.backward()\r\n",
    "            optimizer_en.step()\r\n",
    "\r\n",
    "            if i % 100 == 0:\r\n",
    "                print(\"cn_epoch\",e,\"iteration\",i,loss.item())\r\n",
    "\r\n",
    "torch.save(model_cn, \"./model_cn.pth\")\r\n",
    "torch.save(model_en, \"./model_en.pth\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)"
  },
  "interpreter": {
   "hash": "663e9bff0f8e489cfb11a535201dd029d3f4b5dbba25ed4b671d30c0c2c6bfa9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}